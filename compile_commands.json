[
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/cc -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -o CMakeFiles/neuralnetwork.dir/main.c.o -c /home/nita/dev/c/neural-network/main.c",
  "file": "/home/nita/dev/c/neural-network/main.c",
  "output": "/home/nita/dev/c/neural-network/build/CMakeFiles/neuralnetwork.dir/main.c.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/cc -DGGML_BUILD -DGGML_COMMIT=\\\"2e7e63852\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.9.7\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -o llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml.c",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml.c",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BUILD -DGGML_COMMIT=\\\"2e7e63852\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.9.7\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/cc -DGGML_BUILD -DGGML_COMMIT=\\\"2e7e63852\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.9.7\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -o llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-alloc.c",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-alloc.c",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BUILD -DGGML_COMMIT=\\\"2e7e63852\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.9.7\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-backend.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-backend.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BUILD -DGGML_COMMIT=\\\"2e7e63852\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.9.7\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-opt.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-opt.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BUILD -DGGML_COMMIT=\\\"2e7e63852\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.9.7\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-threading.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-threading.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/cc -DGGML_BUILD -DGGML_COMMIT=\\\"2e7e63852\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.9.7\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -o llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-quants.c",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-quants.c",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BUILD -DGGML_COMMIT=\\\"2e7e63852\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.9.7\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/gguf.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/gguf.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-dl.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-backend-dl.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-backend-dl.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-dl.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-backend-reg.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-backend-reg.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/cc -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -fopenmp -o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.c",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.c",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/repack.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/repack.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/hbm.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/hbm.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/cc -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -fopenmp -o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/quants.c",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/quants.c",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/traits.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/traits.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/amx/amx.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/amx/amx.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/amx/mmq.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/amx/mmq.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/binary-ops.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/binary-ops.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/unary-ops.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/unary-ops.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/vec.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/vec.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/ops.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/ops.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/llamafile/sgemm.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/llamafile/sgemm.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/cc -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -fopenmp -o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/arch/x86/quants.c",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/arch/x86/quants.c",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/arch/x86/repack.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/arch/x86/repack.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/acc.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/acc.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/add-id.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/add-id.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/add-id.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/add-id.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/arange.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/arange.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/argmax.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/argmax.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/argsort.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/argsort.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/binbcast.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/binbcast.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/clamp.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/clamp.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/concat.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/concat.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/conv-transpose-1d.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/conv-transpose-1d.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/conv2d-dw.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/conv2d-dw.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/conv2d-transpose.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/conv2d-transpose.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/conv2d.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/conv2d.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/convert.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/convert.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/count-equal.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/count-equal.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/cpy.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/cpy.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/cross-entropy-loss.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/cross-entropy-loss.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/cumsum.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cumsum.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/cumsum.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cumsum.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/diag.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diag.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/diag.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diag.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/diagmask.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/diagmask.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/fattn-tile.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/fattn-tile.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/fattn-wmma-f16.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/fattn-wmma-f16.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/fattn.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/fattn.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/fill.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fill.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/fill.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fill.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/getrows.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/getrows.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/gla.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/gla.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/im2col.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/im2col.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/mean.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/mean.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/mmf.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmf.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/mmf.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmf.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/mmid.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmid.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/mmid.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmid.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/mmq.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/mmq.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/mmvf.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvf.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/mmvf.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvf.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/mmvq.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/mmvq.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/norm.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/norm.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/opt-step-adamw.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/opt-step-adamw.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/opt-step-sgd.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-sgd.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/opt-step-sgd.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-sgd.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/out-prod.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/out-prod.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/pad.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/pad.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/pad_reflect_1d.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad_reflect_1d.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/pad_reflect_1d.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad_reflect_1d.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/pool2d.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/pool2d.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/quantize.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/quantize.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/roll.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/roll.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/roll.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/roll.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/rope.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/rope.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/scale.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/scale.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/set-rows.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/set-rows.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/set.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/set.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/softcap.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softcap.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/softcap.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softcap.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/softmax.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/softmax.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/solve_tri.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/solve_tri.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/solve_tri.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/solve_tri.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/ssm-conv.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/ssm-conv.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/ssm-scan.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/ssm-scan.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/sum.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/sum.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/sumrows.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/sumrows.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/top-k.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/top-k.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/top-k.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/top-k.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/topk-moe.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/topk-moe.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/topk-moe.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/topk-moe.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/tri.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tri.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/tri.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tri.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/tsembd.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/tsembd.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/unary.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/unary.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/upscale.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/upscale.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/wkv.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/wkv.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq112-dv112.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq112-dv112.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq112-dv112.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq112-dv112.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq128-dv128.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq128-dv128.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq128-dv128.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq128-dv128.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq256-dv256.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq256-dv256.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq256-dv256.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq256-dv256.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq40-dv40.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq40-dv40.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq40-dv40.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq40-dv40.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq576-dv512.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq576-dv512.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq576-dv512.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq576-dv512.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq64-dv64.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq64-dv64.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq64-dv64.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq64-dv64.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq72-dv72.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq72-dv72.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq72-dv72.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq72-dv72.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq80-dv80.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq80-dv80.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq80-dv80.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq80-dv80.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq96-dv96.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq96-dv96.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq96-dv96.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq96-dv96.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_32.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_32.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_32.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_32.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_32.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_32.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_32.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_32.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq1_s.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq1_s.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_s.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_s.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xs.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xs.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xxs.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xxs.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_s.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_s.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_xxs.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_xxs.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_nl.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_nl.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_xs.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_xs.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-mxfp4.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-mxfp4.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-mxfp4.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-mxfp4.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q2_k.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q2_k.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q3_k.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q3_k.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_0.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_0.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_1.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_1.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_k.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_k.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_0.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_0.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_1.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_1.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_k.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_k.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q6_k.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q6_k.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q8_0.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q8_0.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_1.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_1.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_1.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_1.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_10.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_10.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_10.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_10.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_11.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_11.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_11.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_11.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_12.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_12.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_12.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_12.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_13.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_13.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_13.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_13.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_14.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_14.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_14.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_14.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_15.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_15.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_15.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_15.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_16.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_16.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_16.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_16.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_2.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_2.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_2.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_2.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_3.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_3.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_3.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_3.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_4.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_4.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_4.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_4.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_5.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_5.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_5.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_5.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_6.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_6.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_6.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_6.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_7.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_7.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_7.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_7.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_8.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_8.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_8.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_8.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_9.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_9.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_9.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_9.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-instance-q4_0-q4_0.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q4_0.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-instance-q4_0-q4_0.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q4_0.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-instance-q8_0-q8_0.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q8_0.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-instance-q8_0-q8_0.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q8_0.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/opt/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -x cu -c /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-instance-f16-f16.cu -o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-f16.cu.o",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-instance-f16-f16.cu",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-f16.cu.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama-adapter.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama-adapter.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama-arch.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama-arch.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama-batch.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama-batch.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama-chat.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama-chat.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama-context.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama-context.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama-cparams.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama-cparams.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama-cparams.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama-cparams.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama-grammar.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama-grammar.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama-graph.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama-graph.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama-graph.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama-graph.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama-hparams.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama-hparams.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama-impl.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama-impl.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama-io.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama-io.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama-io.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama-io.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama-kv-cache.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama-kv-cache.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama-kv-cache-iswa.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama-kv-cache-iswa.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama-memory.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama-memory.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama-memory.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama-memory.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama-memory-hybrid.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama-memory-hybrid.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid-iswa.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama-memory-hybrid-iswa.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama-memory-hybrid-iswa.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid-iswa.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama-memory-recurrent.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama-memory-recurrent.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama-mmap.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama-mmap.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama-model-loader.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama-model-loader.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama-model-saver.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama-model-saver.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama-model.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama-model.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama-model.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama-model.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama-quant.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama-quant.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama-sampler.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama-sampler.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama-sampler.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama-sampler.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/llama-vocab.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/llama-vocab.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/unicode-data.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/unicode-data.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/unicode.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/unicode.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/afmoe.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/afmoe.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/afmoe.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/afmoe.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/apertus.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/apertus.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/apertus.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/apertus.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/arcee.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/arcee.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/arcee.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/arcee.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/arctic.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/arctic.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/arctic.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/arctic.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/arwkv7.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/arwkv7.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/arwkv7.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/arwkv7.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/baichuan.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/baichuan.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/baichuan.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/baichuan.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/bailingmoe.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/bailingmoe.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/bailingmoe2.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/bailingmoe2.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/bert.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/bert.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/bert.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/bert.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/bitnet.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/bitnet.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/bitnet.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/bitnet.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/bloom.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/bloom.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/bloom.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/bloom.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/chameleon.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/chameleon.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/chameleon.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/chameleon.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/chatglm.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/chatglm.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/chatglm.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/chatglm.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/codeshell.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/codeshell.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/codeshell.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/codeshell.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/cogvlm.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/cogvlm.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/cogvlm.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/cogvlm.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/cohere2-iswa.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/cohere2-iswa.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/command-r.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/command-r.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/command-r.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/command-r.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/dbrx.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/dbrx.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/dbrx.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/dbrx.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/deci.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/deci.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/deci.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/deci.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/deepseek.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/deepseek.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/deepseek.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/deepseek.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/deepseek2.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/deepseek2.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/deepseek2.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/deepseek2.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/delta-net-base.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/delta-net-base.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/delta-net-base.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/delta-net-base.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/dots1.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/dots1.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/dots1.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/dots1.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/dream.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/dream.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/dream.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/dream.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/ernie4-5-moe.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/ernie4-5-moe.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/ernie4-5.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/ernie4-5.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/eurobert.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/eurobert.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/eurobert.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/eurobert.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/exaone-moe.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/exaone-moe.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/exaone-moe.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/exaone-moe.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/exaone.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/exaone.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/exaone.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/exaone.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/exaone4.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/exaone4.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/exaone4.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/exaone4.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/falcon-h1.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/falcon-h1.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/falcon.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/falcon.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/falcon.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/falcon.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/gemma-embedding.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/gemma-embedding.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/gemma.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/gemma.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/gemma.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/gemma.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/gemma2-iswa.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/gemma2-iswa.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/gemma3.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/gemma3.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/gemma3.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/gemma3.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/gemma3n-iswa.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/gemma3n-iswa.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/glm4-moe.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/glm4-moe.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/glm4.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/glm4.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/glm4.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/glm4.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/gpt2.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/gpt2.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/gpt2.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/gpt2.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/gptneox.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/gptneox.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/gptneox.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/gptneox.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/granite-hybrid.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/granite-hybrid.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/granite.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/granite.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/granite.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/granite.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/grok.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/grok.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/grok.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/grok.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/grovemoe.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/grovemoe.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/grovemoe.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/grovemoe.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/hunyuan-dense.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/hunyuan-dense.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/hunyuan-moe.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/hunyuan-moe.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/internlm2.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/internlm2.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/internlm2.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/internlm2.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/jais.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/jais.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/jais.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/jais.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/jais2.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/jais2.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/jais2.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/jais2.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/jamba.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/jamba.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/jamba.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/jamba.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/kimi-linear.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/kimi-linear.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/kimi-linear.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/kimi-linear.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/lfm2.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/lfm2.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/lfm2.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/lfm2.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/llada-moe.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/llada-moe.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/llada-moe.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/llada-moe.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/llada.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/llada.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/llada.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/llada.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/llama-iswa.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/llama-iswa.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/llama.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/llama.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/llama.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/llama.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/maincoder.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/maincoder.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/maincoder.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/maincoder.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/mamba-base.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/mamba-base.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/mamba-base.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/mamba-base.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/mamba.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/mamba.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/mamba.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/mamba.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/mimo2-iswa.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/mimo2-iswa.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/mimo2-iswa.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/mimo2-iswa.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/minicpm3.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/minicpm3.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/minicpm3.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/minicpm3.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/minimax-m2.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/minimax-m2.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/mistral3.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/mistral3.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/mistral3.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/mistral3.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/modern-bert.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/modern-bert.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/modern-bert.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/modern-bert.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/mpt.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/mpt.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/mpt.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/mpt.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/nemotron-h.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/nemotron-h.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/nemotron.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/nemotron.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/nemotron.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/nemotron.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/neo-bert.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/neo-bert.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/neo-bert.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/neo-bert.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/olmo.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/olmo.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/olmo.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/olmo.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/olmo2.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/olmo2.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/olmo2.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/olmo2.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/olmoe.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/olmoe.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/olmoe.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/olmoe.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/openai-moe-iswa.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/openai-moe-iswa.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/openelm.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/openelm.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/openelm.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/openelm.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/orion.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/orion.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/orion.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/orion.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/paddleocr.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/paddleocr.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/paddleocr.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/paddleocr.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/pangu-embedded.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/pangu-embedded.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/phi2.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/phi2.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/phi2.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/phi2.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/phi3.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/phi3.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/phi3.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/phi3.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/plamo.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/plamo.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/plamo.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/plamo.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/plamo2.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/plamo2.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/plamo2.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/plamo2.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/plamo3.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/plamo3.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/plamo3.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/plamo3.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/plm.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/plm.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/plm.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/plm.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/qwen.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/qwen.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/qwen.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/qwen.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/qwen2.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/qwen2.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/qwen2.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/qwen2.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/qwen2moe.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/qwen2moe.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/qwen2vl.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/qwen2vl.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/qwen3.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/qwen3.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/qwen3.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/qwen3.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/qwen35.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/qwen35.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/qwen35.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/qwen35.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/qwen35moe.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/qwen35moe.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/qwen35moe.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/qwen35moe.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/qwen3moe.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/qwen3moe.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/qwen3next.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/qwen3next.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/qwen3next.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/qwen3next.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/qwen3vl-moe.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/qwen3vl-moe.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/qwen3vl.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/qwen3vl.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/refact.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/refact.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/refact.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/refact.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/rnd1.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/rnd1.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/rnd1.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/rnd1.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/rwkv6-base.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/rwkv6-base.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/rwkv6.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/rwkv6.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/rwkv6.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/rwkv6.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/rwkv6qwen2.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/rwkv6qwen2.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/rwkv7-base.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/rwkv7-base.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/rwkv7.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/rwkv7.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/rwkv7.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/rwkv7.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/seed-oss.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/seed-oss.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/seed-oss.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/seed-oss.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/smallthinker.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/smallthinker.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/smallthinker.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/smallthinker.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/smollm3.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/smollm3.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/smollm3.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/smollm3.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/stablelm.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/stablelm.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/stablelm.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/stablelm.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/starcoder.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/starcoder.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/starcoder.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/starcoder.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/starcoder2.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/starcoder2.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/starcoder2.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/starcoder2.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/step35-iswa.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/step35-iswa.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/step35-iswa.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/step35-iswa.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/t5-dec.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/t5-dec.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/t5-dec.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/t5-dec.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/t5-enc.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/t5-enc.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/t5-enc.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/t5-enc.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/wavtokenizer-dec.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/wavtokenizer-dec.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o"
},
{
  "directory": "/home/nita/dev/c/neural-network/build",
  "command": "/usr/sbin/c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -o llama.cpp/src/CMakeFiles/llama.dir/models/xverse.cpp.o -c /home/nita/dev/c/neural-network/llama.cpp/src/models/xverse.cpp",
  "file": "/home/nita/dev/c/neural-network/llama.cpp/src/models/xverse.cpp",
  "output": "/home/nita/dev/c/neural-network/build/llama.cpp/src/CMakeFiles/llama.dir/models/xverse.cpp.o"
}
]
