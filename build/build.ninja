# CMAKE generated file: DO NOT EDIT!
# Generated by "Ninja" Generator, CMake Version 4.2

# This file contains all the build statements describing the
# compilation DAG.

# =============================================================================
# Write statements declared in CMakeLists.txt:
# 
# Which is the root file.
# =============================================================================

# =============================================================================
# Project: neuralnetwork
# Configurations: Release
# =============================================================================

#############################################
# Minimal version of Ninja required by this file

ninja_required_version = 1.5


#############################################
# Set configuration variable for custom commands.

CONFIGURATION = Release
# =============================================================================
# Include auxiliary files.


#############################################
# Include rules file.

include CMakeFiles/rules.ninja

# =============================================================================

#############################################
# Logical path to working directory; prefix for absolute paths.

cmake_ninja_workdir = /home/nita/dev/c/neural-network/build/
# =============================================================================
# Object build statements for EXECUTABLE target neuralnetwork


#############################################
# Order-only phony target for neuralnetwork

build cmake_object_order_depends_target_neuralnetwork: phony || cmake_object_order_depends_target_ggml cmake_object_order_depends_target_ggml-base cmake_object_order_depends_target_ggml-cpu cmake_object_order_depends_target_ggml-cuda cmake_object_order_depends_target_llama

build CMakeFiles/neuralnetwork.dir/main.c.o: C_COMPILER__neuralnetwork_unscanned_Release /home/nita/dev/c/neural-network/main.c || cmake_object_order_depends_target_neuralnetwork
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED
  DEP_FILE = CMakeFiles/neuralnetwork.dir/main.c.o.d
  FLAGS = -O3 -DNDEBUG
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = CMakeFiles/neuralnetwork.dir
  OBJECT_FILE_DIR = CMakeFiles/neuralnetwork.dir
  TARGET_SUPPORT_DIR = CMakeFiles/neuralnetwork.dir


# =============================================================================
# Link build statements for EXECUTABLE target neuralnetwork


#############################################
# Link the executable neuralnetwork

build neuralnetwork: C_EXECUTABLE_LINKER__neuralnetwork_Release CMakeFiles/neuralnetwork.dir/main.c.o | bin/libllama.so.0.0.8173 bin/libggml.so.0.9.7 bin/libggml-cpu.so.0.9.7 bin/libggml-cuda.so.0.9.7 bin/libggml-base.so.0.9.7 /opt/cuda/targets/x86_64-linux/lib/stubs/libcuda.so || bin/libggml-base.so bin/libggml-cpu.so bin/libggml-cuda.so bin/libggml.so bin/libllama.so bin/libllama.so bin/libggml.so bin/libggml-cpu.so bin/libggml-cuda.so bin/libggml-base.so
  CONFIG = Release
  DEP_FILE = CMakeFiles/neuralnetwork.dir/link.d
  FLAGS = -O3 -DNDEBUG
  LINK_FLAGS = -Wl,--dependency-file=CMakeFiles/neuralnetwork.dir/link.d
  LINK_LIBRARIES = -Wl,-rpath,/home/nita/dev/c/neural-network/build/bin  bin/libllama.so.0.0.8173  bin/libggml.so.0.9.7  bin/libggml-cpu.so.0.9.7  bin/libggml-cuda.so.0.9.7  bin/libggml-base.so.0.9.7  /opt/cuda/targets/x86_64-linux/lib/stubs/libcuda.so
  OBJECT_DIR = CMakeFiles/neuralnetwork.dir
  POST_BUILD = :
  PRE_LINK = :
  TARGET_FILE = neuralnetwork
  TARGET_PDB = neuralnetwork.dbg
  TARGET_SUPPORT_DIR = CMakeFiles/neuralnetwork.dir


#############################################
# Utility command for edit_cache

build CMakeFiles/edit_cache.util: CUSTOM_COMMAND
  COMMAND = cd /home/nita/dev/c/neural-network/build && /usr/sbin/ccmake -S/home/nita/dev/c/neural-network -B/home/nita/dev/c/neural-network/build
  DESC = Running CMake cache editor...
  pool = console
  restat = 1

build edit_cache: phony CMakeFiles/edit_cache.util


#############################################
# Utility command for rebuild_cache

build CMakeFiles/rebuild_cache.util: CUSTOM_COMMAND
  COMMAND = cd /home/nita/dev/c/neural-network/build && /usr/sbin/cmake --regenerate-during-build -S/home/nita/dev/c/neural-network -B/home/nita/dev/c/neural-network/build
  DESC = Running CMake to regenerate build system...
  pool = console
  restat = 1

build rebuild_cache: phony CMakeFiles/rebuild_cache.util


#############################################
# Utility command for list_install_components

build list_install_components: phony


#############################################
# Utility command for install

build CMakeFiles/install.util: CUSTOM_COMMAND all
  COMMAND = cd /home/nita/dev/c/neural-network/build && /usr/sbin/cmake -P cmake_install.cmake
  DESC = Install the project...
  pool = console
  restat = 1

build install: phony CMakeFiles/install.util


#############################################
# Utility command for install/local

build CMakeFiles/install/local.util: CUSTOM_COMMAND all
  COMMAND = cd /home/nita/dev/c/neural-network/build && /usr/sbin/cmake -DCMAKE_INSTALL_LOCAL_ONLY=1 -P cmake_install.cmake
  DESC = Installing only the local directory...
  pool = console
  restat = 1

build install/local: phony CMakeFiles/install/local.util


#############################################
# Utility command for install/strip

build CMakeFiles/install/strip.util: CUSTOM_COMMAND all
  COMMAND = cd /home/nita/dev/c/neural-network/build && /usr/sbin/cmake -DCMAKE_INSTALL_DO_STRIP=1 -P cmake_install.cmake
  DESC = Installing the project stripped...
  pool = console
  restat = 1

build install/strip: phony CMakeFiles/install/strip.util

# =============================================================================
# Write statements declared in CMakeLists.txt:
# /home/nita/dev/c/neural-network/CMakeLists.txt
# =============================================================================


#############################################
# Utility command for edit_cache

build llama.cpp/CMakeFiles/edit_cache.util: CUSTOM_COMMAND
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp && /usr/sbin/ccmake -S/home/nita/dev/c/neural-network -B/home/nita/dev/c/neural-network/build
  DESC = Running CMake cache editor...
  pool = console
  restat = 1

build llama.cpp/edit_cache: phony llama.cpp/CMakeFiles/edit_cache.util


#############################################
# Utility command for rebuild_cache

build llama.cpp/CMakeFiles/rebuild_cache.util: CUSTOM_COMMAND
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp && /usr/sbin/cmake --regenerate-during-build -S/home/nita/dev/c/neural-network -B/home/nita/dev/c/neural-network/build
  DESC = Running CMake to regenerate build system...
  pool = console
  restat = 1

build llama.cpp/rebuild_cache: phony llama.cpp/CMakeFiles/rebuild_cache.util


#############################################
# Utility command for list_install_components

build llama.cpp/list_install_components: phony


#############################################
# Utility command for install

build llama.cpp/CMakeFiles/install.util: CUSTOM_COMMAND llama.cpp/all
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp && /usr/sbin/cmake -P cmake_install.cmake
  DESC = Install the project...
  pool = console
  restat = 1

build llama.cpp/install: phony llama.cpp/CMakeFiles/install.util


#############################################
# Utility command for install/local

build llama.cpp/CMakeFiles/install/local.util: CUSTOM_COMMAND llama.cpp/all
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp && /usr/sbin/cmake -DCMAKE_INSTALL_LOCAL_ONLY=1 -P cmake_install.cmake
  DESC = Installing only the local directory...
  pool = console
  restat = 1

build llama.cpp/install/local: phony llama.cpp/CMakeFiles/install/local.util


#############################################
# Utility command for install/strip

build llama.cpp/CMakeFiles/install/strip.util: CUSTOM_COMMAND llama.cpp/all
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp && /usr/sbin/cmake -DCMAKE_INSTALL_DO_STRIP=1 -P cmake_install.cmake
  DESC = Installing the project stripped...
  pool = console
  restat = 1

build llama.cpp/install/strip: phony llama.cpp/CMakeFiles/install/strip.util

# =============================================================================
# Write statements declared in CMakeLists.txt:
# /home/nita/dev/c/neural-network/llama.cpp/CMakeLists.txt
# =============================================================================


#############################################
# Utility command for edit_cache

build llama.cpp/ggml/CMakeFiles/edit_cache.util: CUSTOM_COMMAND
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/ggml && /usr/sbin/ccmake -S/home/nita/dev/c/neural-network -B/home/nita/dev/c/neural-network/build
  DESC = Running CMake cache editor...
  pool = console
  restat = 1

build llama.cpp/ggml/edit_cache: phony llama.cpp/ggml/CMakeFiles/edit_cache.util


#############################################
# Utility command for rebuild_cache

build llama.cpp/ggml/CMakeFiles/rebuild_cache.util: CUSTOM_COMMAND
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/ggml && /usr/sbin/cmake --regenerate-during-build -S/home/nita/dev/c/neural-network -B/home/nita/dev/c/neural-network/build
  DESC = Running CMake to regenerate build system...
  pool = console
  restat = 1

build llama.cpp/ggml/rebuild_cache: phony llama.cpp/ggml/CMakeFiles/rebuild_cache.util


#############################################
# Utility command for list_install_components

build llama.cpp/ggml/list_install_components: phony


#############################################
# Utility command for install

build llama.cpp/ggml/CMakeFiles/install.util: CUSTOM_COMMAND llama.cpp/ggml/all
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/ggml && /usr/sbin/cmake -P cmake_install.cmake
  DESC = Install the project...
  pool = console
  restat = 1

build llama.cpp/ggml/install: phony llama.cpp/ggml/CMakeFiles/install.util


#############################################
# Utility command for install/local

build llama.cpp/ggml/CMakeFiles/install/local.util: CUSTOM_COMMAND llama.cpp/ggml/all
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/ggml && /usr/sbin/cmake -DCMAKE_INSTALL_LOCAL_ONLY=1 -P cmake_install.cmake
  DESC = Installing only the local directory...
  pool = console
  restat = 1

build llama.cpp/ggml/install/local: phony llama.cpp/ggml/CMakeFiles/install/local.util


#############################################
# Utility command for install/strip

build llama.cpp/ggml/CMakeFiles/install/strip.util: CUSTOM_COMMAND llama.cpp/ggml/all
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/ggml && /usr/sbin/cmake -DCMAKE_INSTALL_DO_STRIP=1 -P cmake_install.cmake
  DESC = Installing the project stripped...
  pool = console
  restat = 1

build llama.cpp/ggml/install/strip: phony llama.cpp/ggml/CMakeFiles/install/strip.util

# =============================================================================
# Write statements declared in CMakeLists.txt:
# /home/nita/dev/c/neural-network/llama.cpp/ggml/CMakeLists.txt
# =============================================================================

# =============================================================================
# Object build statements for SHARED_LIBRARY target ggml-base


#############################################
# Order-only phony target for ggml-base

build cmake_object_order_depends_target_ggml-base: phony || .

build llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o: C_COMPILER__ggml-base_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml.c || cmake_object_order_depends_target_ggml-base
  CONFIG = Release
  DEFINES = -DGGML_BUILD -DGGML_COMMIT=\"2e7e63852\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\"0.9.7\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir

build llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o: CXX_COMPILER__ggml-base_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml.cpp || cmake_object_order_depends_target_ggml-base
  CONFIG = Release
  DEFINES = -DGGML_BUILD -DGGML_COMMIT=\"2e7e63852\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\"0.9.7\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir

build llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o: C_COMPILER__ggml-base_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-alloc.c || cmake_object_order_depends_target_ggml-base
  CONFIG = Release
  DEFINES = -DGGML_BUILD -DGGML_COMMIT=\"2e7e63852\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\"0.9.7\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir

build llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o: CXX_COMPILER__ggml-base_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-backend.cpp || cmake_object_order_depends_target_ggml-base
  CONFIG = Release
  DEFINES = -DGGML_BUILD -DGGML_COMMIT=\"2e7e63852\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\"0.9.7\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir

build llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o: CXX_COMPILER__ggml-base_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-opt.cpp || cmake_object_order_depends_target_ggml-base
  CONFIG = Release
  DEFINES = -DGGML_BUILD -DGGML_COMMIT=\"2e7e63852\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\"0.9.7\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir

build llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o: CXX_COMPILER__ggml-base_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-threading.cpp || cmake_object_order_depends_target_ggml-base
  CONFIG = Release
  DEFINES = -DGGML_BUILD -DGGML_COMMIT=\"2e7e63852\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\"0.9.7\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir

build llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o: C_COMPILER__ggml-base_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-quants.c || cmake_object_order_depends_target_ggml-base
  CONFIG = Release
  DEFINES = -DGGML_BUILD -DGGML_COMMIT=\"2e7e63852\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\"0.9.7\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir

build llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o: CXX_COMPILER__ggml-base_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/gguf.cpp || cmake_object_order_depends_target_ggml-base
  CONFIG = Release
  DEFINES = -DGGML_BUILD -DGGML_COMMIT=\"2e7e63852\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\"0.9.7\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir


# =============================================================================
# Link build statements for SHARED_LIBRARY target ggml-base


#############################################
# Link the shared library bin/libggml-base.so.0.9.7

build bin/libggml-base.so.0.9.7: CXX_SHARED_LIBRARY_LINKER__ggml-base_Release llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
  CONFIG = Release
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/link.d
  LANGUAGE_COMPILE_FLAGS = -O3 -DNDEBUG
  LINK_FLAGS = -shared -Wl,--dependency-file=llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/link.d
  LINK_LIBRARIES = -lm
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir
  POST_BUILD = :
  PRE_LINK = :
  SONAME = libggml-base.so.0
  SONAME_FLAG = -Wl,-soname,
  TARGET_FILE = bin/libggml-base.so.0.9.7
  TARGET_IMPLIB = llama.cpp/ggml/src/
  TARGET_PDB = ggml-base.so.dbg
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-base.dir


#############################################
# Create library symlink bin/libggml-base.so

build bin/libggml-base.so.0 bin/libggml-base.so: CMAKE_SYMLINK_LIBRARY bin/libggml-base.so.0.9.7
  POST_BUILD = :

# =============================================================================
# Object build statements for SHARED_LIBRARY target ggml


#############################################
# Order-only phony target for ggml

build cmake_object_order_depends_target_ggml: phony || cmake_object_order_depends_target_ggml-base cmake_object_order_depends_target_ggml-cpu cmake_object_order_depends_target_ggml-cuda

build llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-dl.cpp.o: CXX_COMPILER__ggml_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-backend-dl.cpp || cmake_object_order_depends_target_ggml
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-dl.cpp.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml.dir

build llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o: CXX_COMPILER__ggml_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-backend-reg.cpp || cmake_object_order_depends_target_ggml
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml.dir


# =============================================================================
# Link build statements for SHARED_LIBRARY target ggml


#############################################
# Link the shared library bin/libggml.so.0.9.7

build bin/libggml.so.0.9.7: CXX_SHARED_LIBRARY_LINKER__ggml_Release llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-dl.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o | bin/libggml-cpu.so.0.9.7 bin/libggml-cuda.so.0.9.7 bin/libggml-base.so.0.9.7 /opt/cuda/targets/x86_64-linux/lib/stubs/libcuda.so || bin/libggml-base.so bin/libggml-cpu.so bin/libggml-cuda.so bin/libggml-cpu.so bin/libggml-cuda.so bin/libggml-base.so
  CONFIG = Release
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml.dir/link.d
  LANGUAGE_COMPILE_FLAGS = -O3 -DNDEBUG
  LINK_FLAGS = -shared -Wl,--dependency-file=llama.cpp/ggml/src/CMakeFiles/ggml.dir/link.d
  LINK_LIBRARIES = -Wl,-rpath,/home/nita/dev/c/neural-network/build/bin:  -ldl  bin/libggml-cpu.so.0.9.7  bin/libggml-cuda.so.0.9.7  bin/libggml-base.so.0.9.7  /opt/cuda/targets/x86_64-linux/lib/stubs/libcuda.so  -ldl
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml.dir
  POST_BUILD = :
  PRE_LINK = :
  SONAME = libggml.so.0
  SONAME_FLAG = -Wl,-soname,
  TARGET_FILE = bin/libggml.so.0.9.7
  TARGET_IMPLIB = llama.cpp/ggml/src/
  TARGET_PDB = ggml.so.dbg
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml.dir


#############################################
# Create library symlink bin/libggml.so

build bin/libggml.so.0 bin/libggml.so: CMAKE_SYMLINK_LIBRARY bin/libggml.so.0.9.7
  POST_BUILD = :

# =============================================================================
# Object build statements for SHARED_LIBRARY target ggml-cpu


#############################################
# Order-only phony target for ggml-cpu

build cmake_object_order_depends_target_ggml-cpu: phony || cmake_object_order_depends_target_ggml-base

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o: C_COMPILER__ggml-cpu_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.c || cmake_object_order_depends_target_ggml-cpu
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -fopenmp
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o: CXX_COMPILER__ggml-cpu_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.cpp || cmake_object_order_depends_target_ggml-cpu
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o: CXX_COMPILER__ggml-cpu_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/repack.cpp || cmake_object_order_depends_target_ggml-cpu
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o: CXX_COMPILER__ggml-cpu_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/hbm.cpp || cmake_object_order_depends_target_ggml-cpu
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o: C_COMPILER__ggml-cpu_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/quants.c || cmake_object_order_depends_target_ggml-cpu
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -fopenmp
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o: CXX_COMPILER__ggml-cpu_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/traits.cpp || cmake_object_order_depends_target_ggml-cpu
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o: CXX_COMPILER__ggml-cpu_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/amx/amx.cpp || cmake_object_order_depends_target_ggml-cpu
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o: CXX_COMPILER__ggml-cpu_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/amx/mmq.cpp || cmake_object_order_depends_target_ggml-cpu
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o: CXX_COMPILER__ggml-cpu_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/binary-ops.cpp || cmake_object_order_depends_target_ggml-cpu
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o: CXX_COMPILER__ggml-cpu_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/unary-ops.cpp || cmake_object_order_depends_target_ggml-cpu
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o: CXX_COMPILER__ggml-cpu_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/vec.cpp || cmake_object_order_depends_target_ggml-cpu
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o: CXX_COMPILER__ggml-cpu_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/ops.cpp || cmake_object_order_depends_target_ggml-cpu
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o: CXX_COMPILER__ggml-cpu_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/llamafile/sgemm.cpp || cmake_object_order_depends_target_ggml-cpu
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o: C_COMPILER__ggml-cpu_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/arch/x86/quants.c || cmake_object_order_depends_target_ggml-cpu
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -fopenmp
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir

build llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o: CXX_COMPILER__ggml-cpu_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/arch/x86/repack.cpp || cmake_object_order_depends_target_ggml-cpu
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o.d
  FLAGS = -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir


# =============================================================================
# Link build statements for SHARED_LIBRARY target ggml-cpu


#############################################
# Link the shared library bin/libggml-cpu.so.0.9.7

build bin/libggml-cpu.so.0.9.7: CXX_SHARED_LIBRARY_LINKER__ggml-cpu_Release llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o | bin/libggml-base.so.0.9.7 /usr/lib/libgomp.so /usr/lib/libpthread.a || bin/libggml-base.so bin/libggml-base.so
  CONFIG = Release
  DEP_FILE = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/link.d
  LANGUAGE_COMPILE_FLAGS = -O3 -DNDEBUG
  LINK_FLAGS = -shared -Wl,--dependency-file=llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/link.d
  LINK_LIBRARIES = -Wl,-rpath,/home/nita/dev/c/neural-network/build/bin:  bin/libggml-base.so.0.9.7  /usr/lib/libgomp.so  /usr/lib/libpthread.a
  OBJECT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir
  POST_BUILD = :
  PRE_LINK = :
  SONAME = libggml-cpu.so.0
  SONAME_FLAG = -Wl,-soname,
  TARGET_FILE = bin/libggml-cpu.so.0.9.7
  TARGET_IMPLIB = llama.cpp/ggml/src/
  TARGET_PDB = ggml-cpu.so.dbg
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir


#############################################
# Create library symlink bin/libggml-cpu.so

build bin/libggml-cpu.so.0 bin/libggml-cpu.so: CMAKE_SYMLINK_LIBRARY bin/libggml-cpu.so.0.9.7
  POST_BUILD = :


#############################################
# Utility command for edit_cache

build llama.cpp/ggml/src/CMakeFiles/edit_cache.util: CUSTOM_COMMAND
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/ggml/src && /usr/sbin/ccmake -S/home/nita/dev/c/neural-network -B/home/nita/dev/c/neural-network/build
  DESC = Running CMake cache editor...
  pool = console
  restat = 1

build llama.cpp/ggml/src/edit_cache: phony llama.cpp/ggml/src/CMakeFiles/edit_cache.util


#############################################
# Utility command for rebuild_cache

build llama.cpp/ggml/src/CMakeFiles/rebuild_cache.util: CUSTOM_COMMAND
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/ggml/src && /usr/sbin/cmake --regenerate-during-build -S/home/nita/dev/c/neural-network -B/home/nita/dev/c/neural-network/build
  DESC = Running CMake to regenerate build system...
  pool = console
  restat = 1

build llama.cpp/ggml/src/rebuild_cache: phony llama.cpp/ggml/src/CMakeFiles/rebuild_cache.util


#############################################
# Utility command for list_install_components

build llama.cpp/ggml/src/list_install_components: phony


#############################################
# Utility command for install

build llama.cpp/ggml/src/CMakeFiles/install.util: CUSTOM_COMMAND llama.cpp/ggml/src/all
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/ggml/src && /usr/sbin/cmake -P cmake_install.cmake
  DESC = Install the project...
  pool = console
  restat = 1

build llama.cpp/ggml/src/install: phony llama.cpp/ggml/src/CMakeFiles/install.util


#############################################
# Utility command for install/local

build llama.cpp/ggml/src/CMakeFiles/install/local.util: CUSTOM_COMMAND llama.cpp/ggml/src/all
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/ggml/src && /usr/sbin/cmake -DCMAKE_INSTALL_LOCAL_ONLY=1 -P cmake_install.cmake
  DESC = Installing only the local directory...
  pool = console
  restat = 1

build llama.cpp/ggml/src/install/local: phony llama.cpp/ggml/src/CMakeFiles/install/local.util


#############################################
# Utility command for install/strip

build llama.cpp/ggml/src/CMakeFiles/install/strip.util: CUSTOM_COMMAND llama.cpp/ggml/src/all
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/ggml/src && /usr/sbin/cmake -DCMAKE_INSTALL_DO_STRIP=1 -P cmake_install.cmake
  DESC = Installing the project stripped...
  pool = console
  restat = 1

build llama.cpp/ggml/src/install/strip: phony llama.cpp/ggml/src/CMakeFiles/install/strip.util

# =============================================================================
# Write statements declared in CMakeLists.txt:
# /home/nita/dev/c/neural-network/llama.cpp/ggml/src/CMakeLists.txt
# =============================================================================


#############################################
# Utility command for edit_cache

build llama.cpp/ggml/src/ggml-cpu/CMakeFiles/edit_cache.util: CUSTOM_COMMAND
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cpu && /usr/sbin/ccmake -S/home/nita/dev/c/neural-network -B/home/nita/dev/c/neural-network/build
  DESC = Running CMake cache editor...
  pool = console
  restat = 1

build llama.cpp/ggml/src/ggml-cpu/edit_cache: phony llama.cpp/ggml/src/ggml-cpu/CMakeFiles/edit_cache.util


#############################################
# Utility command for rebuild_cache

build llama.cpp/ggml/src/ggml-cpu/CMakeFiles/rebuild_cache.util: CUSTOM_COMMAND
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cpu && /usr/sbin/cmake --regenerate-during-build -S/home/nita/dev/c/neural-network -B/home/nita/dev/c/neural-network/build
  DESC = Running CMake to regenerate build system...
  pool = console
  restat = 1

build llama.cpp/ggml/src/ggml-cpu/rebuild_cache: phony llama.cpp/ggml/src/ggml-cpu/CMakeFiles/rebuild_cache.util


#############################################
# Utility command for list_install_components

build llama.cpp/ggml/src/ggml-cpu/list_install_components: phony


#############################################
# Utility command for install

build llama.cpp/ggml/src/ggml-cpu/CMakeFiles/install.util: CUSTOM_COMMAND llama.cpp/ggml/src/ggml-cpu/all
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cpu && /usr/sbin/cmake -P cmake_install.cmake
  DESC = Install the project...
  pool = console
  restat = 1

build llama.cpp/ggml/src/ggml-cpu/install: phony llama.cpp/ggml/src/ggml-cpu/CMakeFiles/install.util


#############################################
# Utility command for install/local

build llama.cpp/ggml/src/ggml-cpu/CMakeFiles/install/local.util: CUSTOM_COMMAND llama.cpp/ggml/src/ggml-cpu/all
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cpu && /usr/sbin/cmake -DCMAKE_INSTALL_LOCAL_ONLY=1 -P cmake_install.cmake
  DESC = Installing only the local directory...
  pool = console
  restat = 1

build llama.cpp/ggml/src/ggml-cpu/install/local: phony llama.cpp/ggml/src/ggml-cpu/CMakeFiles/install/local.util


#############################################
# Utility command for install/strip

build llama.cpp/ggml/src/ggml-cpu/CMakeFiles/install/strip.util: CUSTOM_COMMAND llama.cpp/ggml/src/ggml-cpu/all
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cpu && /usr/sbin/cmake -DCMAKE_INSTALL_DO_STRIP=1 -P cmake_install.cmake
  DESC = Installing the project stripped...
  pool = console
  restat = 1

build llama.cpp/ggml/src/ggml-cpu/install/strip: phony llama.cpp/ggml/src/ggml-cpu/CMakeFiles/install/strip.util

# =============================================================================
# Write statements declared in CMakeLists.txt:
# /home/nita/dev/c/neural-network/llama.cpp/ggml/src/CMakeLists.txt
# =============================================================================

# =============================================================================
# Object build statements for SHARED_LIBRARY target ggml-cuda


#############################################
# Order-only phony target for ggml-cuda

build cmake_object_order_depends_target_ggml-cuda: phony || cmake_object_order_depends_target_ggml-base

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/acc.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/add-id.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/add-id.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/add-id.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/arange.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/argmax.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/argsort.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/binbcast.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/clamp.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/concat.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/conv-transpose-1d.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/conv2d-dw.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/conv2d-transpose.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/conv2d.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/convert.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/count-equal.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/cpy.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/cross-entropy-loss.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cumsum.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/cumsum.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cumsum.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diag.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/diag.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diag.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/diagmask.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/fattn-tile.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/fattn-wmma-f16.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/fattn.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fill.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/fill.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fill.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/getrows.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/gla.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/im2col.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/mean.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmf.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/mmf.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmf.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmid.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/mmid.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmid.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/mmq.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvf.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/mmvf.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvf.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/mmvq.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/norm.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/opt-step-adamw.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-sgd.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/opt-step-sgd.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-sgd.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/out-prod.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/pad.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad_reflect_1d.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/pad_reflect_1d.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad_reflect_1d.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/pool2d.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/quantize.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/roll.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/roll.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/roll.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/rope.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/scale.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/set-rows.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/set.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softcap.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/softcap.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softcap.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/softmax.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/solve_tri.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/solve_tri.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/solve_tri.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/ssm-conv.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/ssm-scan.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/sum.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/sumrows.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/top-k.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/top-k.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/top-k.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/topk-moe.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/topk-moe.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/topk-moe.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tri.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/tri.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tri.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/tsembd.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/unary.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/upscale.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/wkv.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq112-dv112.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq112-dv112.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq112-dv112.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq128-dv128.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq128-dv128.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq128-dv128.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq256-dv256.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq256-dv256.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq256-dv256.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq40-dv40.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq40-dv40.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq40-dv40.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq576-dv512.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq576-dv512.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq576-dv512.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq64-dv64.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq64-dv64.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq64-dv64.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq72-dv72.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq72-dv72.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq72-dv72.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq80-dv80.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq80-dv80.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq80-dv80.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq96-dv96.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq96-dv96.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq96-dv96.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_32.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_32.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_32.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_32.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_32.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_32.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq1_s.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_s.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xs.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xxs.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_s.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_xxs.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_nl.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_xs.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-mxfp4.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-mxfp4.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-mxfp4.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q2_k.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q3_k.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_0.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_1.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_k.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_0.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_1.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_k.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q6_k.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q8_0.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_1.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_1.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_1.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_10.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_10.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_10.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_11.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_11.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_11.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_12.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_12.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_12.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_13.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_13.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_13.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_14.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_14.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_14.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_15.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_15.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_15.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_16.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_16.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_16.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_2.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_2.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_2.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_3.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_3.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_3.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_4.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_4.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_4.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_5.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_5.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_5.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_6.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_6.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_6.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_7.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_7.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_7.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_8.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_8.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_8.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_9.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/mmf-instance-ncols_9.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_9.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q4_0.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-instance-q4_0-q4_0.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q4_0.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q8_0.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-instance-q8_0-q8_0.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q8_0.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-f16.cu.o: CUDA_COMPILER__ggml-cuda_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-instance-f16-f16.cu || cmake_object_order_depends_target_ggml-cuda
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS
  DEP_FILE = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-f16.cu.o.d
  FLAGS = -O3 -DNDEBUG -std=c++17 "--generate-code=arch=compute_86,code=[sm_86]" -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic"
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/.. -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include -isystem /opt/cuda/targets/x86_64-linux/include -isystem /opt/cuda/targets/x86_64-linux/include/cccl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  OBJECT_FILE_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir


# =============================================================================
# Link build statements for SHARED_LIBRARY target ggml-cuda


#############################################
# Link the shared library bin/libggml-cuda.so.0.9.7

build bin/libggml-cuda.so.0.9.7: CUDA_SHARED_LIBRARY_LINKER__ggml-cuda_Release llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/add-id.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cumsum.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diag.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fill.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmf.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmid.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvf.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-sgd.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad_reflect_1d.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/roll.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softcap.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/solve_tri.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/top-k.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/topk-moe.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tri.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq112-dv112.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq128-dv128.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq256-dv256.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq40-dv40.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq576-dv512.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq64-dv64.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq72-dv72.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq80-dv80.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq96-dv96.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_32.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_32.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-mxfp4.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_1.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_10.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_11.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_12.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_13.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_14.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_15.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_16.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_2.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_3.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_4.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_5.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_6.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_7.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_8.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_9.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q4_0.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q8_0.cu.o llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-f16.cu.o | bin/libggml-base.so.0.9.7 /opt/cuda/targets/x86_64-linux/lib/libcudart.so /opt/cuda/targets/x86_64-linux/lib/libcublas.so /opt/cuda/targets/x86_64-linux/lib/stubs/libcuda.so /opt/cuda/targets/x86_64-linux/lib/libcublasLt.so /opt/cuda/targets/x86_64-linux/lib/libculibos.a /usr/lib/librt.a || bin/libggml-base.so bin/libggml-base.so
  CONFIG = Release
  LANGUAGE_COMPILE_FLAGS = -O3 -DNDEBUG "--generate-code=arch=compute_86,code=[sm_86]"
  LINK_FLAGS = -shared
  LINK_LIBRARIES = -Wl,-rpath,/home/nita/dev/c/neural-network/build/bin:  bin/libggml-base.so.0.9.7  /opt/cuda/targets/x86_64-linux/lib/libcudart.so  /opt/cuda/targets/x86_64-linux/lib/libcublas.so  /opt/cuda/targets/x86_64-linux/lib/stubs/libcuda.so  /opt/cuda/targets/x86_64-linux/lib/libcublasLt.so  /opt/cuda/targets/x86_64-linux/lib/libculibos.a  -ldl  /usr/lib/librt.a  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl
  OBJECT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir
  POST_BUILD = :
  PRE_LINK = :
  SONAME = libggml-cuda.so.0
  SONAME_FLAG = -Wl,-soname,
  TARGET_FILE = bin/libggml-cuda.so.0.9.7
  TARGET_IMPLIB = llama.cpp/ggml/src/ggml-cuda/
  TARGET_PDB = ggml-cuda.so.dbg
  TARGET_SUPPORT_DIR = llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir


#############################################
# Create library symlink bin/libggml-cuda.so

build bin/libggml-cuda.so.0 bin/libggml-cuda.so: CMAKE_SYMLINK_LIBRARY bin/libggml-cuda.so.0.9.7
  POST_BUILD = :


#############################################
# Utility command for edit_cache

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/edit_cache.util: CUSTOM_COMMAND
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda && /usr/sbin/ccmake -S/home/nita/dev/c/neural-network -B/home/nita/dev/c/neural-network/build
  DESC = Running CMake cache editor...
  pool = console
  restat = 1

build llama.cpp/ggml/src/ggml-cuda/edit_cache: phony llama.cpp/ggml/src/ggml-cuda/CMakeFiles/edit_cache.util


#############################################
# Utility command for rebuild_cache

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/rebuild_cache.util: CUSTOM_COMMAND
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda && /usr/sbin/cmake --regenerate-during-build -S/home/nita/dev/c/neural-network -B/home/nita/dev/c/neural-network/build
  DESC = Running CMake to regenerate build system...
  pool = console
  restat = 1

build llama.cpp/ggml/src/ggml-cuda/rebuild_cache: phony llama.cpp/ggml/src/ggml-cuda/CMakeFiles/rebuild_cache.util


#############################################
# Utility command for list_install_components

build llama.cpp/ggml/src/ggml-cuda/list_install_components: phony


#############################################
# Utility command for install

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/install.util: CUSTOM_COMMAND llama.cpp/ggml/src/ggml-cuda/all
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda && /usr/sbin/cmake -P cmake_install.cmake
  DESC = Install the project...
  pool = console
  restat = 1

build llama.cpp/ggml/src/ggml-cuda/install: phony llama.cpp/ggml/src/ggml-cuda/CMakeFiles/install.util


#############################################
# Utility command for install/local

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/install/local.util: CUSTOM_COMMAND llama.cpp/ggml/src/ggml-cuda/all
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda && /usr/sbin/cmake -DCMAKE_INSTALL_LOCAL_ONLY=1 -P cmake_install.cmake
  DESC = Installing only the local directory...
  pool = console
  restat = 1

build llama.cpp/ggml/src/ggml-cuda/install/local: phony llama.cpp/ggml/src/ggml-cuda/CMakeFiles/install/local.util


#############################################
# Utility command for install/strip

build llama.cpp/ggml/src/ggml-cuda/CMakeFiles/install/strip.util: CUSTOM_COMMAND llama.cpp/ggml/src/ggml-cuda/all
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda && /usr/sbin/cmake -DCMAKE_INSTALL_DO_STRIP=1 -P cmake_install.cmake
  DESC = Installing the project stripped...
  pool = console
  restat = 1

build llama.cpp/ggml/src/ggml-cuda/install/strip: phony llama.cpp/ggml/src/ggml-cuda/CMakeFiles/install/strip.util

# =============================================================================
# Write statements declared in CMakeLists.txt:
# /home/nita/dev/c/neural-network/llama.cpp/CMakeLists.txt
# =============================================================================

# =============================================================================
# Object build statements for SHARED_LIBRARY target llama


#############################################
# Order-only phony target for llama

build cmake_object_order_depends_target_llama: phony || cmake_object_order_depends_target_ggml cmake_object_order_depends_target_ggml-base cmake_object_order_depends_target_ggml-cpu cmake_object_order_depends_target_ggml-cuda

build llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama-adapter.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama-arch.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama-batch.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama-chat.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama-context.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/llama-cparams.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama-cparams.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-cparams.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama-grammar.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/llama-graph.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama-graph.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-graph.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama-hparams.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama-impl.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/llama-io.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama-io.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-io.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama-kv-cache.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama-kv-cache-iswa.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/llama-memory.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama-memory.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-memory.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama-memory-hybrid.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid-iswa.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama-memory-hybrid-iswa.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid-iswa.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama-memory-recurrent.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama-mmap.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama-model-loader.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama-model-saver.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/llama-model.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama-model.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-model.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama-quant.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/llama-sampler.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama-sampler.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-sampler.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/llama-vocab.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/unicode-data.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/unicode.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/afmoe.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/afmoe.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/afmoe.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/apertus.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/apertus.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/apertus.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/arcee.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/arcee.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/arcee.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/arctic.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/arctic.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/arctic.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/arwkv7.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/arwkv7.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/arwkv7.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/baichuan.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/baichuan.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/baichuan.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/bailingmoe.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/bailingmoe2.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/bert.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/bert.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/bert.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/bitnet.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/bitnet.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/bitnet.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/bloom.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/bloom.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/bloom.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/chameleon.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/chameleon.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/chameleon.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/chatglm.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/chatglm.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/chatglm.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/codeshell.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/codeshell.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/codeshell.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/cogvlm.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/cogvlm.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/cogvlm.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/cohere2-iswa.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/command-r.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/command-r.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/command-r.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/dbrx.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/dbrx.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/dbrx.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/deci.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/deci.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/deci.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/deepseek.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/deepseek.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/deepseek.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/deepseek2.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/deepseek2.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/deepseek2.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/delta-net-base.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/delta-net-base.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/delta-net-base.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/dots1.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/dots1.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/dots1.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/dream.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/dream.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/dream.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/ernie4-5-moe.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/ernie4-5.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/eurobert.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/eurobert.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/eurobert.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/exaone-moe.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/exaone-moe.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/exaone-moe.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/exaone.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/exaone.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/exaone.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/exaone4.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/exaone4.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/exaone4.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/falcon-h1.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/falcon.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/falcon.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/falcon.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/gemma-embedding.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/gemma.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/gemma.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/gemma.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/gemma2-iswa.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/gemma3.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/gemma3.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/gemma3.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/gemma3n-iswa.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/glm4-moe.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/glm4.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/glm4.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/glm4.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/gpt2.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/gpt2.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/gpt2.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/gptneox.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/gptneox.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/gptneox.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/granite-hybrid.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/granite.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/granite.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/granite.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/grok.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/grok.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/grok.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/grovemoe.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/grovemoe.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/grovemoe.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/hunyuan-dense.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/hunyuan-moe.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/internlm2.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/internlm2.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/internlm2.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/jais.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/jais.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/jais.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/jais2.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/jais2.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/jais2.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/jamba.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/jamba.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/jamba.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/kimi-linear.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/kimi-linear.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/kimi-linear.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/lfm2.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/lfm2.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/lfm2.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/llada-moe.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/llada-moe.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/llada-moe.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/llada.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/llada.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/llada.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/llama-iswa.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/llama.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/llama.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/llama.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/maincoder.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/maincoder.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/maincoder.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/mamba-base.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/mamba-base.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/mamba-base.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/mamba.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/mamba.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/mamba.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/mimo2-iswa.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/mimo2-iswa.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/mimo2-iswa.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/minicpm3.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/minicpm3.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/minicpm3.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/minimax-m2.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/mistral3.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/mistral3.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/mistral3.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/modern-bert.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/modern-bert.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/modern-bert.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/mpt.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/mpt.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/mpt.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/nemotron-h.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/nemotron.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/nemotron.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/nemotron.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/neo-bert.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/neo-bert.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/neo-bert.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/olmo.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/olmo.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/olmo.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/olmo2.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/olmo2.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/olmo2.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/olmoe.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/olmoe.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/olmoe.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/openai-moe-iswa.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/openelm.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/openelm.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/openelm.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/orion.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/orion.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/orion.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/paddleocr.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/paddleocr.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/paddleocr.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/pangu-embedded.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/phi2.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/phi2.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/phi2.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/phi3.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/phi3.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/phi3.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/plamo.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/plamo.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/plamo.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/plamo2.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/plamo2.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/plamo2.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/plamo3.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/plamo3.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/plamo3.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/plm.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/plm.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/plm.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/qwen.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/qwen.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/qwen.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/qwen2.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/qwen2.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/qwen2.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/qwen2moe.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/qwen2vl.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/qwen3.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/qwen3.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/qwen3.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/qwen35.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/qwen35.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/qwen35.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/qwen35moe.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/qwen35moe.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/qwen35moe.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/qwen3moe.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/qwen3next.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/qwen3next.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/qwen3next.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/qwen3vl-moe.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/qwen3vl.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/refact.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/refact.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/refact.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/rnd1.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/rnd1.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/rnd1.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/rwkv6-base.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/rwkv6.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/rwkv6.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/rwkv6.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/rwkv6qwen2.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/rwkv7-base.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/rwkv7.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/rwkv7.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/rwkv7.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/seed-oss.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/seed-oss.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/seed-oss.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/smallthinker.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/smallthinker.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/smallthinker.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/smollm3.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/smollm3.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/smollm3.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/stablelm.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/stablelm.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/stablelm.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/starcoder.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/starcoder.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/starcoder.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/starcoder2.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/starcoder2.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/starcoder2.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/step35-iswa.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/step35-iswa.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/step35-iswa.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/t5-dec.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/t5-dec.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/t5-dec.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/t5-enc.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/t5-enc.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/t5-enc.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/wavtokenizer-dec.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir

build llama.cpp/src/CMakeFiles/llama.dir/models/xverse.cpp.o: CXX_COMPILER__llama_unscanned_Release /home/nita/dev/c/neural-network/llama.cpp/src/models/xverse.cpp || cmake_object_order_depends_target_llama
  CONFIG = Release
  DEFINES = -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/models/xverse.cpp.o.d
  FLAGS = -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi
  INCLUDES = -I/home/nita/dev/c/neural-network/llama.cpp/src/. -I/home/nita/dev/c/neural-network/llama.cpp/src/../include -I/home/nita/dev/c/neural-network/llama.cpp/ggml/src/../include
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  OBJECT_FILE_DIR = llama.cpp/src/CMakeFiles/llama.dir/models
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir


# =============================================================================
# Link build statements for SHARED_LIBRARY target llama


#############################################
# Link the shared library bin/libllama.so.0.0.8173

build bin/libllama.so.0.0.8173: CXX_SHARED_LIBRARY_LINKER__llama_Release llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-cparams.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-graph.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-io.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-memory.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid-iswa.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-model.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-sampler.cpp.o llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o llama.cpp/src/CMakeFiles/llama.dir/unicode.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/afmoe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/apertus.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/arcee.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/arctic.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/arwkv7.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/baichuan.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/bert.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/bitnet.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/bloom.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/chameleon.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/chatglm.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/codeshell.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/cogvlm.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/command-r.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/dbrx.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/deci.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/deepseek.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/deepseek2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/delta-net-base.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/dots1.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/dream.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/eurobert.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/exaone-moe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/exaone.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/exaone4.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/falcon.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/gemma.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/gemma3.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/glm4.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/gpt2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/gptneox.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/granite.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/grok.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/grovemoe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/internlm2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/jais.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/jais2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/jamba.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/kimi-linear.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/lfm2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/llada-moe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/llada.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/llama.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/maincoder.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/mamba-base.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/mamba.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/mimo2-iswa.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/minicpm3.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/mistral3.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/modern-bert.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/mpt.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/nemotron.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/neo-bert.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/olmo.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/olmo2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/olmoe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/openelm.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/orion.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/paddleocr.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/phi2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/phi3.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/plamo.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/plamo2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/plamo3.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/plm.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/qwen.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/qwen2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/qwen3.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/qwen35.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/qwen35moe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/qwen3next.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/refact.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/rnd1.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/rwkv6.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/rwkv7.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/seed-oss.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/smallthinker.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/smollm3.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/stablelm.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/starcoder.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/starcoder2.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/step35-iswa.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/t5-dec.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/t5-enc.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o llama.cpp/src/CMakeFiles/llama.dir/models/xverse.cpp.o | bin/libggml.so.0.9.7 bin/libggml-cpu.so.0.9.7 bin/libggml-cuda.so.0.9.7 bin/libggml-base.so.0.9.7 /opt/cuda/targets/x86_64-linux/lib/stubs/libcuda.so || bin/libggml-base.so bin/libggml-cpu.so bin/libggml-cuda.so bin/libggml.so bin/libggml.so bin/libggml-cpu.so bin/libggml-cuda.so bin/libggml-base.so
  CONFIG = Release
  DEP_FILE = llama.cpp/src/CMakeFiles/llama.dir/link.d
  LANGUAGE_COMPILE_FLAGS = -O3 -DNDEBUG
  LINK_FLAGS = -shared -Wl,--dependency-file=llama.cpp/src/CMakeFiles/llama.dir/link.d
  LINK_LIBRARIES = -Wl,-rpath,/home/nita/dev/c/neural-network/build/bin:  bin/libggml.so.0.9.7  bin/libggml-cpu.so.0.9.7  bin/libggml-cuda.so.0.9.7  bin/libggml-base.so.0.9.7  /opt/cuda/targets/x86_64-linux/lib/stubs/libcuda.so
  OBJECT_DIR = llama.cpp/src/CMakeFiles/llama.dir
  POST_BUILD = :
  PRE_LINK = :
  SONAME = libllama.so.0
  SONAME_FLAG = -Wl,-soname,
  TARGET_FILE = bin/libllama.so.0.0.8173
  TARGET_IMPLIB = llama.cpp/src/
  TARGET_PDB = llama.so.dbg
  TARGET_SUPPORT_DIR = llama.cpp/src/CMakeFiles/llama.dir


#############################################
# Create library symlink bin/libllama.so

build bin/libllama.so.0 bin/libllama.so: CMAKE_SYMLINK_LIBRARY bin/libllama.so.0.0.8173
  POST_BUILD = :


#############################################
# Utility command for edit_cache

build llama.cpp/src/CMakeFiles/edit_cache.util: CUSTOM_COMMAND
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/src && /usr/sbin/ccmake -S/home/nita/dev/c/neural-network -B/home/nita/dev/c/neural-network/build
  DESC = Running CMake cache editor...
  pool = console
  restat = 1

build llama.cpp/src/edit_cache: phony llama.cpp/src/CMakeFiles/edit_cache.util


#############################################
# Utility command for rebuild_cache

build llama.cpp/src/CMakeFiles/rebuild_cache.util: CUSTOM_COMMAND
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/src && /usr/sbin/cmake --regenerate-during-build -S/home/nita/dev/c/neural-network -B/home/nita/dev/c/neural-network/build
  DESC = Running CMake to regenerate build system...
  pool = console
  restat = 1

build llama.cpp/src/rebuild_cache: phony llama.cpp/src/CMakeFiles/rebuild_cache.util


#############################################
# Utility command for list_install_components

build llama.cpp/src/list_install_components: phony


#############################################
# Utility command for install

build llama.cpp/src/CMakeFiles/install.util: CUSTOM_COMMAND llama.cpp/src/all
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/src && /usr/sbin/cmake -P cmake_install.cmake
  DESC = Install the project...
  pool = console
  restat = 1

build llama.cpp/src/install: phony llama.cpp/src/CMakeFiles/install.util


#############################################
# Utility command for install/local

build llama.cpp/src/CMakeFiles/install/local.util: CUSTOM_COMMAND llama.cpp/src/all
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/src && /usr/sbin/cmake -DCMAKE_INSTALL_LOCAL_ONLY=1 -P cmake_install.cmake
  DESC = Installing only the local directory...
  pool = console
  restat = 1

build llama.cpp/src/install/local: phony llama.cpp/src/CMakeFiles/install/local.util


#############################################
# Utility command for install/strip

build llama.cpp/src/CMakeFiles/install/strip.util: CUSTOM_COMMAND llama.cpp/src/all
  COMMAND = cd /home/nita/dev/c/neural-network/build/llama.cpp/src && /usr/sbin/cmake -DCMAKE_INSTALL_DO_STRIP=1 -P cmake_install.cmake
  DESC = Installing the project stripped...
  pool = console
  restat = 1

build llama.cpp/src/install/strip: phony llama.cpp/src/CMakeFiles/install/strip.util

# =============================================================================
# Target aliases.

build ggml: phony bin/libggml.so

build ggml-base: phony bin/libggml-base.so

build ggml-cpu: phony bin/libggml-cpu.so

build ggml-cuda: phony bin/libggml-cuda.so

build libggml-base.so: phony bin/libggml-base.so

build libggml-cpu.so: phony bin/libggml-cpu.so

build libggml-cuda.so: phony bin/libggml-cuda.so

build libggml.so: phony bin/libggml.so

build libllama.so: phony bin/libllama.so

build llama: phony bin/libllama.so

# =============================================================================
# Folder targets.

# =============================================================================

#############################################
# Folder: /home/nita/dev/c/neural-network/build

build all: phony neuralnetwork llama.cpp/all

# =============================================================================

#############################################
# Folder: /home/nita/dev/c/neural-network/build/llama.cpp

build llama.cpp/all: phony llama.cpp/ggml/all llama.cpp/src/all

# =============================================================================

#############################################
# Folder: /home/nita/dev/c/neural-network/build/llama.cpp/ggml

build llama.cpp/ggml/all: phony llama.cpp/ggml/src/all

# =============================================================================

#############################################
# Folder: /home/nita/dev/c/neural-network/build/llama.cpp/ggml/src

build llama.cpp/ggml/src/all: phony bin/libggml-base.so bin/libggml.so bin/libggml-cpu.so llama.cpp/ggml/src/ggml-cpu/all llama.cpp/ggml/src/ggml-cuda/all

# =============================================================================

#############################################
# Folder: /home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cpu

build llama.cpp/ggml/src/ggml-cpu/all: phony

# =============================================================================

#############################################
# Folder: /home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda

build llama.cpp/ggml/src/ggml-cuda/all: phony bin/libggml-cuda.so

# =============================================================================

#############################################
# Folder: /home/nita/dev/c/neural-network/build/llama.cpp/src

build llama.cpp/src/all: phony bin/libllama.so

# =============================================================================
# Built-in targets


#############################################
# Re-run CMake if any of its inputs changed.

build build.ninja /home/nita/dev/c/neural-network/build/cmake_install.cmake /home/nita/dev/c/neural-network/build/llama.cpp/cmake_install.cmake /home/nita/dev/c/neural-network/build/llama.cpp/ggml/cmake_install.cmake /home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/cmake_install.cmake /home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cpu/cmake_install.cmake /home/nita/dev/c/neural-network/build/llama.cpp/ggml/src/ggml-cuda/cmake_install.cmake /home/nita/dev/c/neural-network/build/llama.cpp/src/cmake_install.cmake: RERUN_CMAKE | /home/nita/dev/c/neural-network/CMakeLists.txt /home/nita/dev/c/neural-network/llama.cpp/CMakeLists.txt /home/nita/dev/c/neural-network/llama.cpp/cmake/build-info.cmake /home/nita/dev/c/neural-network/llama.cpp/cmake/common.cmake /home/nita/dev/c/neural-network/llama.cpp/cmake/license.cmake /home/nita/dev/c/neural-network/llama.cpp/cmake/llama-config.cmake.in /home/nita/dev/c/neural-network/llama.cpp/cmake/llama.pc.in /home/nita/dev/c/neural-network/llama.cpp/ggml/CMakeLists.txt /home/nita/dev/c/neural-network/llama.cpp/ggml/cmake/common.cmake /home/nita/dev/c/neural-network/llama.cpp/ggml/cmake/ggml-config.cmake.in /home/nita/dev/c/neural-network/llama.cpp/ggml/src/CMakeLists.txt /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/CMakeLists.txt /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/CMakeLists.txt /home/nita/dev/c/neural-network/llama.cpp/src/CMakeLists.txt /usr/share/cmake/Modules/BasicConfigVersion-SameMajorVersion.cmake.in /usr/share/cmake/Modules/CMakeASMInformation.cmake /usr/share/cmake/Modules/CMakeCInformation.cmake /usr/share/cmake/Modules/CMakeCUDAInformation.cmake /usr/share/cmake/Modules/CMakeCXXInformation.cmake /usr/share/cmake/Modules/CMakeCheckCompilerFlagCommonPatterns.cmake /usr/share/cmake/Modules/CMakeCommonLanguageInclude.cmake /usr/share/cmake/Modules/CMakeGenericSystem.cmake /usr/share/cmake/Modules/CMakeInitializeConfigs.cmake /usr/share/cmake/Modules/CMakeLanguageInformation.cmake /usr/share/cmake/Modules/CMakePackageConfigHelpers.cmake /usr/share/cmake/Modules/CMakeParseImplicitLinkInfo.cmake /usr/share/cmake/Modules/CMakeSystemSpecificInformation.cmake /usr/share/cmake/Modules/CMakeSystemSpecificInitialize.cmake /usr/share/cmake/Modules/CheckCSourceCompiles.cmake /usr/share/cmake/Modules/CheckCXXCompilerFlag.cmake /usr/share/cmake/Modules/CheckCXXSourceCompiles.cmake /usr/share/cmake/Modules/CheckIncludeFile.cmake /usr/share/cmake/Modules/CheckIncludeFileCXX.cmake /usr/share/cmake/Modules/CheckLibraryExists.cmake /usr/share/cmake/Modules/Compiler/CMakeCommonCompilerMacros.cmake /usr/share/cmake/Modules/Compiler/GNU-ASM.cmake /usr/share/cmake/Modules/Compiler/GNU-C.cmake /usr/share/cmake/Modules/Compiler/GNU-CXX.cmake /usr/share/cmake/Modules/Compiler/GNU.cmake /usr/share/cmake/Modules/Compiler/NVIDIA-CUDA.cmake /usr/share/cmake/Modules/Compiler/NVIDIA.cmake /usr/share/cmake/Modules/FindCUDAToolkit.cmake /usr/share/cmake/Modules/FindGit.cmake /usr/share/cmake/Modules/FindOpenMP.cmake /usr/share/cmake/Modules/FindPackageHandleStandardArgs.cmake /usr/share/cmake/Modules/FindPackageMessage.cmake /usr/share/cmake/Modules/FindThreads.cmake /usr/share/cmake/Modules/GNUInstallDirs.cmake /usr/share/cmake/Modules/Internal/CMakeASMLinkerInformation.cmake /usr/share/cmake/Modules/Internal/CMakeCLinkerInformation.cmake /usr/share/cmake/Modules/Internal/CMakeCXXLinkerInformation.cmake /usr/share/cmake/Modules/Internal/CMakeCommonLinkerInformation.cmake /usr/share/cmake/Modules/Internal/CheckCompilerFlag.cmake /usr/share/cmake/Modules/Internal/CheckFlagCommonConfig.cmake /usr/share/cmake/Modules/Internal/CheckSourceCompiles.cmake /usr/share/cmake/Modules/Linker/GNU-C.cmake /usr/share/cmake/Modules/Linker/GNU-CXX.cmake /usr/share/cmake/Modules/Linker/GNU.cmake /usr/share/cmake/Modules/Platform/Linker/GNU.cmake /usr/share/cmake/Modules/Platform/Linker/Linux-ASM.cmake /usr/share/cmake/Modules/Platform/Linker/Linux-GNU-ASM.cmake /usr/share/cmake/Modules/Platform/Linker/Linux-GNU-C.cmake /usr/share/cmake/Modules/Platform/Linker/Linux-GNU-CXX.cmake /usr/share/cmake/Modules/Platform/Linker/Linux-GNU.cmake /usr/share/cmake/Modules/Platform/Linux-GNU-C.cmake /usr/share/cmake/Modules/Platform/Linux-GNU-CXX.cmake /usr/share/cmake/Modules/Platform/Linux-GNU.cmake /usr/share/cmake/Modules/Platform/Linux-Initialize.cmake /usr/share/cmake/Modules/Platform/Linux-NVIDIA-CUDA.cmake /usr/share/cmake/Modules/Platform/Linux.cmake /usr/share/cmake/Modules/Platform/UnixPaths.cmake /usr/share/cmake/Modules/WriteBasicConfigVersionFile.cmake CMakeCache.txt CMakeFiles/4.2.3/CMakeASMCompiler.cmake CMakeFiles/4.2.3/CMakeCCompiler.cmake CMakeFiles/4.2.3/CMakeCUDACompiler.cmake CMakeFiles/4.2.3/CMakeCXXCompiler.cmake CMakeFiles/4.2.3/CMakeSystem.cmake
  pool = console


#############################################
# A missing CMake input file is not an error.

build /home/nita/dev/c/neural-network/CMakeLists.txt /home/nita/dev/c/neural-network/llama.cpp/CMakeLists.txt /home/nita/dev/c/neural-network/llama.cpp/cmake/build-info.cmake /home/nita/dev/c/neural-network/llama.cpp/cmake/common.cmake /home/nita/dev/c/neural-network/llama.cpp/cmake/license.cmake /home/nita/dev/c/neural-network/llama.cpp/cmake/llama-config.cmake.in /home/nita/dev/c/neural-network/llama.cpp/cmake/llama.pc.in /home/nita/dev/c/neural-network/llama.cpp/ggml/CMakeLists.txt /home/nita/dev/c/neural-network/llama.cpp/ggml/cmake/common.cmake /home/nita/dev/c/neural-network/llama.cpp/ggml/cmake/ggml-config.cmake.in /home/nita/dev/c/neural-network/llama.cpp/ggml/src/CMakeLists.txt /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cpu/CMakeLists.txt /home/nita/dev/c/neural-network/llama.cpp/ggml/src/ggml-cuda/CMakeLists.txt /home/nita/dev/c/neural-network/llama.cpp/src/CMakeLists.txt /usr/share/cmake/Modules/BasicConfigVersion-SameMajorVersion.cmake.in /usr/share/cmake/Modules/CMakeASMInformation.cmake /usr/share/cmake/Modules/CMakeCInformation.cmake /usr/share/cmake/Modules/CMakeCUDAInformation.cmake /usr/share/cmake/Modules/CMakeCXXInformation.cmake /usr/share/cmake/Modules/CMakeCheckCompilerFlagCommonPatterns.cmake /usr/share/cmake/Modules/CMakeCommonLanguageInclude.cmake /usr/share/cmake/Modules/CMakeGenericSystem.cmake /usr/share/cmake/Modules/CMakeInitializeConfigs.cmake /usr/share/cmake/Modules/CMakeLanguageInformation.cmake /usr/share/cmake/Modules/CMakePackageConfigHelpers.cmake /usr/share/cmake/Modules/CMakeParseImplicitLinkInfo.cmake /usr/share/cmake/Modules/CMakeSystemSpecificInformation.cmake /usr/share/cmake/Modules/CMakeSystemSpecificInitialize.cmake /usr/share/cmake/Modules/CheckCSourceCompiles.cmake /usr/share/cmake/Modules/CheckCXXCompilerFlag.cmake /usr/share/cmake/Modules/CheckCXXSourceCompiles.cmake /usr/share/cmake/Modules/CheckIncludeFile.cmake /usr/share/cmake/Modules/CheckIncludeFileCXX.cmake /usr/share/cmake/Modules/CheckLibraryExists.cmake /usr/share/cmake/Modules/Compiler/CMakeCommonCompilerMacros.cmake /usr/share/cmake/Modules/Compiler/GNU-ASM.cmake /usr/share/cmake/Modules/Compiler/GNU-C.cmake /usr/share/cmake/Modules/Compiler/GNU-CXX.cmake /usr/share/cmake/Modules/Compiler/GNU.cmake /usr/share/cmake/Modules/Compiler/NVIDIA-CUDA.cmake /usr/share/cmake/Modules/Compiler/NVIDIA.cmake /usr/share/cmake/Modules/FindCUDAToolkit.cmake /usr/share/cmake/Modules/FindGit.cmake /usr/share/cmake/Modules/FindOpenMP.cmake /usr/share/cmake/Modules/FindPackageHandleStandardArgs.cmake /usr/share/cmake/Modules/FindPackageMessage.cmake /usr/share/cmake/Modules/FindThreads.cmake /usr/share/cmake/Modules/GNUInstallDirs.cmake /usr/share/cmake/Modules/Internal/CMakeASMLinkerInformation.cmake /usr/share/cmake/Modules/Internal/CMakeCLinkerInformation.cmake /usr/share/cmake/Modules/Internal/CMakeCXXLinkerInformation.cmake /usr/share/cmake/Modules/Internal/CMakeCommonLinkerInformation.cmake /usr/share/cmake/Modules/Internal/CheckCompilerFlag.cmake /usr/share/cmake/Modules/Internal/CheckFlagCommonConfig.cmake /usr/share/cmake/Modules/Internal/CheckSourceCompiles.cmake /usr/share/cmake/Modules/Linker/GNU-C.cmake /usr/share/cmake/Modules/Linker/GNU-CXX.cmake /usr/share/cmake/Modules/Linker/GNU.cmake /usr/share/cmake/Modules/Platform/Linker/GNU.cmake /usr/share/cmake/Modules/Platform/Linker/Linux-ASM.cmake /usr/share/cmake/Modules/Platform/Linker/Linux-GNU-ASM.cmake /usr/share/cmake/Modules/Platform/Linker/Linux-GNU-C.cmake /usr/share/cmake/Modules/Platform/Linker/Linux-GNU-CXX.cmake /usr/share/cmake/Modules/Platform/Linker/Linux-GNU.cmake /usr/share/cmake/Modules/Platform/Linux-GNU-C.cmake /usr/share/cmake/Modules/Platform/Linux-GNU-CXX.cmake /usr/share/cmake/Modules/Platform/Linux-GNU.cmake /usr/share/cmake/Modules/Platform/Linux-Initialize.cmake /usr/share/cmake/Modules/Platform/Linux-NVIDIA-CUDA.cmake /usr/share/cmake/Modules/Platform/Linux.cmake /usr/share/cmake/Modules/Platform/UnixPaths.cmake /usr/share/cmake/Modules/WriteBasicConfigVersionFile.cmake CMakeCache.txt CMakeFiles/4.2.3/CMakeASMCompiler.cmake CMakeFiles/4.2.3/CMakeCCompiler.cmake CMakeFiles/4.2.3/CMakeCUDACompiler.cmake CMakeFiles/4.2.3/CMakeCXXCompiler.cmake CMakeFiles/4.2.3/CMakeSystem.cmake: phony


#############################################
# Clean all the built files.

build clean: CLEAN


#############################################
# Print all primary targets available.

build help: HELP


#############################################
# Make the all target the default.

default all
